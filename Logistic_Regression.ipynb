{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression Lab1 Code File.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMHjtVPbyaKP"
      },
      "source": [
        "## Logistic Regression Model for Divorce Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kktr-4GPI5ou"
      },
      "source": [
        "## Part 1.1: Implement  logistic regression from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJi26z8awmSD"
      },
      "source": [
        "### Logistic regression\n",
        "Logistic regression uses an equation as the representation, very much like linear regression.\n",
        "\n",
        "Input values (x) are combined linearly using weights or coefficient values (referred to as W) to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a continuous value.<br>\n",
        "\n",
        "###  $\\hat{y}(w, x) = \\frac{1}{1+exp^{-(w_0 + w_1 * x_1 + ... + w_p * x_p)}}$\n",
        "\n",
        "#### Dataset\n",
        "The dataset is available at <strong>\"data/divorce.csv\"</strong> in the respective challenge's repo.<br>\n",
        "<strong>Original Source:</strong> https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set. Dataset is based on rating for questionnaire filled by people who already got divorse and those who is happily married.<br><br>\n",
        "\n",
        "[//]: # \"The dataset is available at http://archive.ics.uci.edu/ml/machine-learning-databases/00520/data.zip. Unzip the file and use either CSV or xlsx file.<br>\"\n",
        "\n",
        "\n",
        "#### Features (X)\n",
        "1. Atr1 - If one of us apologizes when our discussion deteriorates, the discussion ends. (Numeric | Range: 0-4)\n",
        "2. Atr2 - I know we can ignore our differences, even if things get hard sometimes. (Numeric | Range: 0-4)\n",
        "3. Atr3 - When we need it, we can take our discussions with my spouse from the beginning and correct it. (Numeric | Range: 0-4)\n",
        "4. Atr4 - When I discuss with my spouse, to contact him will eventually work. (Numeric | Range: 0-4)\n",
        "5. Atr5 - The time I spent with my wife is special for us. (Numeric | Range: 0-4)\n",
        "6. Atr6 - We don't have time at home as partners. (Numeric | Range: 0-4)\n",
        "7. Atr7 - We are like two strangers who share the same environment at home rather than family. (Numeric | Range: 0-4)\n",
        "\n",
        "&emsp;.<br>\n",
        "&emsp;.<br>\n",
        "&emsp;.<br>\n",
        "<br>\n",
        "54. Atr54 - I'm not afraid to tell my spouse about her/his incompetence. (Numeric | Range: 0-4)\n",
        "<br><br>\n",
        "Take a look above at the source of the original dataset for more details.\n",
        "\n",
        "#### Target (y)\n",
        "55. Class: (Binary | 1 => Divorced, 0 => Not divorced yet)\n",
        "\n",
        "#### Objective\n",
        "To gain understanding of logistic regression through implementing the model from scratch\n",
        "\n",
        "#### Tasks\n",
        "- Download and load the data (csv file contains ';' as delimiter)\n",
        "- Add column at position 0 with all values=1 (pandas.DataFrame.insert function). This is for input to the bias $w_0$\n",
        "- Define X matrix (independent features) and y vector (target feature) as numpy arrays\n",
        "- Print the shape and datatype of both X and y\n",
        "[//]: # \"- Dataset contains missing values, hence fill the missing values (NA) by performing missing value prediction\"\n",
        "[//]: # \"- Since the all the features are in higher range, columns can be normalized into smaller scale (like 0 to 1) using different methods such as scaling, standardizing or any other suitable preprocessing technique (sklearn.preprocessing.StandardScaler)\"\n",
        "- Split the dataset into 85% for training and rest 15% for testing (sklearn.model_selection.train_test_split function)\n",
        "- Follow logistic regression class and fill code where highlighted:\n",
        "    - Write sigmoid function to predict probabilities\n",
        "    - Write cross entropy or log loss function (i.e. negative log likelihood)\n",
        "    - Write fit function where gradient descent is implemented\n",
        "    - Write predict_proba function where we predict probabilities for input data\n",
        "- Train the model\n",
        "- Write function for calculating accuracy\n",
        "- Compute accuracy on train and test data\n",
        "\n",
        "#### Further Fun (will not be evaluated)\n",
        "- Play with learning rate and max_iterations\n",
        "- Preprocess data with different feature scaling methods (i.e. scaling, normalization, standardization, etc) and observe accuracies on both X_train and X_test\n",
        "- Train model on different train-test splits such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc. and observe accuracies on both X_train and X_test\n",
        "- Shuffle training samples with different random seed values in the train_test_split function. Check the model error for the testing data for each setup.\n",
        "- Print other classification metrics such as:\n",
        "    - classification report (sklearn.metrics.classification_report),\n",
        "    - confusion matrix (sklearn.metrics.confusion_matrix),\n",
        "    - precision, recall and f1 scores (sklearn.metrics.precision_recall_fscore_support)\n",
        "\n",
        "#### Helpful links\n",
        "- How Logistic Regression works: https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
        "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "- Training testing splitting: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21J6cpd_wmSE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SL1fdNt1k3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d33f23-79b2-4531-ae5e-a7d66be6ee4b"
      },
      "source": [
        "# Download the dataset from the source\n",
        "!wget \"https://raw.githubusercontent.com/ramahanisha-7/Datasets/main/Dataset.csv\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-20 14:12:40--  https://raw.githubusercontent.com/ramahanisha-7/Datasets/main/Dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19021 (19K) [text/plain]\n",
            "Saving to: ‘Dataset.csv’\n",
            "\n",
            "\rDataset.csv           0%[                    ]       0  --.-KB/s               \rDataset.csv         100%[===================>]  18.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-09-20 14:12:40 (79.6 MB/s) - ‘Dataset.csv’ saved [19021/19021]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9av7W-wowmSI"
      },
      "source": [
        "# Read the data from local cloud directory\n",
        "data = pd.read_csv(\"Dataset.csv\",sep=\";\")\n",
        "# Set delimiter to semicolon(;) in case of unexpected results"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmXolHwEI5o8"
      },
      "source": [
        "# Add column which has all 1s\n",
        "# The idea is that weight corresponding to this column is equal to intercept\n",
        "# This way it is efficient and easier to handle the bias/intercept term\n",
        "data.insert(0,\"bias\",1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV1jGAQxwmSP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "ae05c6b9-0bdb-442d-f395-8b267e063897"
      },
      "source": [
        "# Print the dataframe rows just to see some samples\n",
        "data.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bias</th>\n",
              "      <th>Atr1</th>\n",
              "      <th>Atr2</th>\n",
              "      <th>Atr3</th>\n",
              "      <th>Atr4</th>\n",
              "      <th>Atr5</th>\n",
              "      <th>Atr6</th>\n",
              "      <th>Atr7</th>\n",
              "      <th>Atr8</th>\n",
              "      <th>Atr9</th>\n",
              "      <th>Atr10</th>\n",
              "      <th>Atr11</th>\n",
              "      <th>Atr12</th>\n",
              "      <th>Atr13</th>\n",
              "      <th>Atr14</th>\n",
              "      <th>Atr15</th>\n",
              "      <th>Atr16</th>\n",
              "      <th>Atr17</th>\n",
              "      <th>Atr18</th>\n",
              "      <th>Atr19</th>\n",
              "      <th>Atr20</th>\n",
              "      <th>Atr21</th>\n",
              "      <th>Atr22</th>\n",
              "      <th>Atr23</th>\n",
              "      <th>Atr24</th>\n",
              "      <th>Atr25</th>\n",
              "      <th>Atr26</th>\n",
              "      <th>Atr27</th>\n",
              "      <th>Atr28</th>\n",
              "      <th>Atr29</th>\n",
              "      <th>Atr30</th>\n",
              "      <th>Atr31</th>\n",
              "      <th>Atr32</th>\n",
              "      <th>Atr33</th>\n",
              "      <th>Atr34</th>\n",
              "      <th>Atr35</th>\n",
              "      <th>Atr36</th>\n",
              "      <th>Atr37</th>\n",
              "      <th>Atr38</th>\n",
              "      <th>Atr39</th>\n",
              "      <th>Atr40</th>\n",
              "      <th>Atr41</th>\n",
              "      <th>Atr42</th>\n",
              "      <th>Atr43</th>\n",
              "      <th>Atr44</th>\n",
              "      <th>Atr45</th>\n",
              "      <th>Atr46</th>\n",
              "      <th>Atr47</th>\n",
              "      <th>Atr48</th>\n",
              "      <th>Atr49</th>\n",
              "      <th>Atr50</th>\n",
              "      <th>Atr51</th>\n",
              "      <th>Atr52</th>\n",
              "      <th>Atr53</th>\n",
              "      <th>Atr54</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bias  Atr1  Atr2  Atr3  Atr4  Atr5  ...  Atr50  Atr51  Atr52  Atr53  Atr54  Class\n",
              "0     1     2     2     4     1     0  ...      3      2      3      2      1      1\n",
              "1     1     4     4     4     4     4  ...      4      4      4      2      2      1\n",
              "2     1     2     2     2     2     1  ...      1      1      2      2      2      1\n",
              "3     1     3     2     3     2     3  ...      3      3      2      2      2      1\n",
              "4     1     2     2     1     1     1  ...      2      2      2      1      0      1\n",
              "5     1     0     0     1     0     0  ...      1      1      1      2      0      1\n",
              "6     1     3     3     3     2     1  ...      3      3      2      2      2      1\n",
              "7     1     2     1     2     2     2  ...      2      1      1      1      0      1\n",
              "8     1     2     2     1     0     0  ...      1      1      1      1      1      1\n",
              "9     1     1     1     1     1     1  ...      2      2      4      3      3      1\n",
              "\n",
              "[10 rows x 56 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KA1b_7O16HO",
        "outputId": "fe977761-46e4-41a2-b541-e52de8b56b6e"
      },
      "source": [
        "df=np.array(data[:])\n",
        "print(df)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 2 ... 2 1 1]\n",
            " [1 4 4 ... 2 2 1]\n",
            " [1 2 2 ... 2 2 1]\n",
            " ...\n",
            " [1 1 1 ... 0 0 0]\n",
            " [1 0 0 ... 3 1 0]\n",
            " [1 0 0 ... 3 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joRU6dWxwmSR"
      },
      "source": [
        "# Define X (input features) and y (output feature) \n",
        "X = df[:,:-1]\n",
        "y = df[:,-1:]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAyM-CYCwmSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd317fc2-f080-430d-fd12-d6bd71fadd6e"
      },
      "source": [
        "X_shape = X.shape\n",
        "X_type  = type(X)\n",
        "y_shape = y.shape\n",
        "y_type  = type(y)\n",
        "print(f'X: Type-{X_type}, Shape-{X_shape}')\n",
        "print(f'y: Type-{y_type}, Shape-{y_shape}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: Type-<class 'numpy.ndarray'>, Shape-(170, 55)\n",
            "y: Type-<class 'numpy.ndarray'>, Shape-(170, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJrEelFzI5pA"
      },
      "source": [
        "<strong>Expected output: </strong><br><br>\n",
        "\n",
        "X: Type-<class 'numpy.ndarray'>, Shape-(170, 55)<br>\n",
        "y: Type-<class 'numpy.ndarray'>, Shape-(170,)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdLIVOm127-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd5cce1d-1bd1-4ab5-898e-4f6898cb0838"
      },
      "source": [
        "# Check and fill any missing values if any\n",
        "data.isnull().sum()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bias     0\n",
              "Atr1     0\n",
              "Atr2     0\n",
              "Atr3     0\n",
              "Atr4     0\n",
              "Atr5     0\n",
              "Atr6     0\n",
              "Atr7     0\n",
              "Atr8     0\n",
              "Atr9     0\n",
              "Atr10    0\n",
              "Atr11    0\n",
              "Atr12    0\n",
              "Atr13    0\n",
              "Atr14    0\n",
              "Atr15    0\n",
              "Atr16    0\n",
              "Atr17    0\n",
              "Atr18    0\n",
              "Atr19    0\n",
              "Atr20    0\n",
              "Atr21    0\n",
              "Atr22    0\n",
              "Atr23    0\n",
              "Atr24    0\n",
              "Atr25    0\n",
              "Atr26    0\n",
              "Atr27    0\n",
              "Atr28    0\n",
              "Atr29    0\n",
              "Atr30    0\n",
              "Atr31    0\n",
              "Atr32    0\n",
              "Atr33    0\n",
              "Atr34    0\n",
              "Atr35    0\n",
              "Atr36    0\n",
              "Atr37    0\n",
              "Atr38    0\n",
              "Atr39    0\n",
              "Atr40    0\n",
              "Atr41    0\n",
              "Atr42    0\n",
              "Atr43    0\n",
              "Atr44    0\n",
              "Atr45    0\n",
              "Atr46    0\n",
              "Atr47    0\n",
              "Atr48    0\n",
              "Atr49    0\n",
              "Atr50    0\n",
              "Atr51    0\n",
              "Atr52    0\n",
              "Atr53    0\n",
              "Atr54    0\n",
              "Class    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En9Kb9dh2-wm"
      },
      "source": [
        "# Perform standarization (if required)\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8WF-EqO3BEa"
      },
      "source": [
        "# Split the dataset into training and testing here\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=2)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acCATJhI3FdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e18a8eb-403e-4a6d-d4a1-9fa0f4c07e24"
      },
      "source": [
        "# Print the shape of features and target of training and testing: X_train, X_test, y_train, y_test\n",
        "X_train_shape = X_train.shape\n",
        "y_train_shape = y_train.shape\n",
        "X_test_shape  = X_test.shape\n",
        "y_test_shape  = y_test.shape\n",
        "\n",
        "print(f\"X_train: {X_train_shape} , y_train: {y_train_shape}\")\n",
        "print(f\"X_test: {X_test_shape} , y_test: {y_test_shape}\")\n",
        "assert (X_train.shape[0]==y_train.shape[0] and X_test.shape[0]==y_test.shape[0]), \"Check your splitting carefully\""
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (144, 55) , y_train: (144, 1)\n",
            "X_test: (26, 55) , y_test: (26, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSa7cW-NwmSd"
      },
      "source": [
        "##### Let us start implementing logistic regression from scratch. Just follow code cells, see hints if required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzxCdzxBI5pF"
      },
      "source": [
        "##### We will build a LogisticRegression class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQgpspPcI5pH"
      },
      "source": [
        "# DO NOT EDIT ANY VARIABLE OR FUNCTION NAME(S) IN THIS CELL\n",
        "# Let's try more object oriented approach this time :)\n",
        "class MyLogisticRegression:\n",
        "    def __init__(self, learning_rate=0.001, max_iterations=1000):\n",
        "        '''Initialize variables\n",
        "        Args:\n",
        "            learning_rate  : Learning Rate\n",
        "            max_iterations : Max iterations for training weights\n",
        "        '''\n",
        "        # Initialising all the parameters\n",
        "        self.learning_rate  = learning_rate\n",
        "        self.max_iterations = max_iterations\n",
        "        self.cross_entropy_error    = [] # Summary of the cross entroy or log loss (i.e. negative log-likelihood)\n",
        "        \n",
        "        # Define epsilon because log(0) is not defined\n",
        "        self.eps = 1e-7\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        '''Sigmoid function: f:R->(0,1)\n",
        "        Args:\n",
        "            z : A numpy array (num_samples,)\n",
        "        Returns:\n",
        "            A numpy array where sigmoid function applied to every element\n",
        "        '''\n",
        "        ### START CODE HERE\n",
        "        sig_z = 1/(1+np.exp(-z))\n",
        "        ### END CODE HERE\n",
        "        \n",
        "        assert (z.shape==sig_z.shape), 'Error in sigmoid implementation. Check carefully'\n",
        "        return sig_z\n",
        "    \n",
        "    def cross_entropy(self, y_true, y_pred):\n",
        "        '''Calculates cross_entropy or log loss (negative log-likelihood) estimate\n",
        "        Remember: -[y * log(yh) + (1-y) * log(1-yh)]\n",
        "        Note: Cross_entropy or log-loss is defined for multiple classes as well, but for this dataset\n",
        "        \n",
        "        Args:\n",
        "            y_true : Numpy array of actual truth values (num_samples,)\n",
        "            y_pred : Numpy array of predicted values (num_samples,)\n",
        "        Returns:\n",
        "            cross_entropy(or log loss which is the negative Log-likelihood) scalar value\n",
        "        '''\n",
        "        ### START CODE HERE\n",
        "        n = y_true.shape[0]\n",
        "        cross_entropy = (-1/n)*(np.sum((y_true*np.log(y_pred)) + ((1-y_true)*np.log(1-y_pred))))\n",
        "        ### END CODE HERE\n",
        "        \n",
        "        return cross_entropy\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        '''Trains logistic regression model using gradient descent\n",
        "        to gain minimum cross_entropy/log-loss on the training data\n",
        "        Args:\n",
        "            X : Numpy array (num_examples, num_features)\n",
        "            y : Numpy array (num_examples, )\n",
        "        Returns: VOID\n",
        "        '''\n",
        "        \n",
        "        num_examples = X.shape[0]\n",
        "        num_features = X.shape[1]\n",
        "        ### START CODE HERE\n",
        "        \n",
        "        # Initialize weights with appropriate shape\n",
        "        # self.weights = np.random.rand(1, num_features)\n",
        "        self.weights = np.ones((1, num_features))\n",
        "        \n",
        "        # Perform gradient descent\n",
        "        for i in range(self.max_iterations):\n",
        "            # Define the linear hypothesis(z) first\n",
        "            # HINT: what is our hypothesis function in linear regression, remember?\n",
        "            z = (1/num_examples)*np.dot(X, self.weights.T)\n",
        "\n",
        "            # Output probability value by appplying sigmoid on z\n",
        "            y_pred = self.sigmoid(z)\n",
        "            # print(y_pred)\n",
        "            # print(y_pred.shape)\n",
        "            \n",
        "            # Calculate the gradient values\n",
        "            # This is just vectorized efficient way of implementing gradient. Don't worry, we will discuss it later.\n",
        "            gradient = np.mean((y-y_pred).T*X.T, axis=1).reshape(1,num_features)\n",
        "            # print(gradient)\n",
        "            # print(gradient.shape)\n",
        "            # Update the weights using gradient descent\n",
        "            self.weights = self.weights - self.learning_rate*gradient\n",
        "            \n",
        "            # Calculating cross entropy or log-loss (negatie log likelihood)\n",
        "            cross_entropy = self.cross_entropy(y, y_pred)\n",
        "\n",
        "            self.cross_entropy_error.append(cross_entropy)\n",
        "    \n",
        "        ### END CODE HERE\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        '''Predict probabilities for given X.\n",
        "        Remember sigmoid returns value between 0 and 1.\n",
        "        Args:\n",
        "            X : Numpy array (num_samples, num_features)\n",
        "        Returns:\n",
        "            probabilities: Numpy array (num_samples,)\n",
        "        '''\n",
        "        if self.weights is None:\n",
        "            raise Exception(\"Fit the model before prediction\")\n",
        "        \n",
        "        ### START CODE HERE\n",
        "        n = X.shape[0]\n",
        "        m = X.shape[1]\n",
        "        z = (1/n)*np.dot(X, self.weights.T)\n",
        "        probabilities = self.sigmoid(z)\n",
        "        ### END CODE HERE\n",
        "        \n",
        "        return probabilities\n",
        "    \n",
        "    def predict(self, X, threshold=0.7):\n",
        "        '''Predict/Classify X in classes\n",
        "        Args:\n",
        "            X         : Numpy array (num_samples, num_features)\n",
        "            threshold : scalar value above which prediction is 1 else 0\n",
        "        Returns:\n",
        "            binary_predictions : Numpy array (num_samples,)\n",
        "        '''\n",
        "        # Thresholding probability to predict binary values\n",
        "        binary_predictions = np.array(list(map(lambda x: 1 if x>threshold else 0, self.predict_proba(X))))\n",
        "        # predictions = self.predict_proba(X)\n",
        "        \n",
        "        return binary_predictions"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crl6mGEoI5pJ"
      },
      "source": [
        "# Now initialize logitic regression implemented by you\n",
        "model = MyLogisticRegression()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmMnrzPBI5pK"
      },
      "source": [
        "# And now fit on training data\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK6tFOQTI5pL"
      },
      "source": [
        "##### Phew!! That's a lot of code. But you did it, congrats !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTZoph5y17an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9f791b-2e11-4dbb-ec71-55a34e06804e"
      },
      "source": [
        " model.predict_proba(X_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.56433977],\n",
              "       [0.72740078],\n",
              "       [0.5562431 ],\n",
              "       [0.73711605],\n",
              "       [0.55303946],\n",
              "       [0.71382097],\n",
              "       [0.5544189 ],\n",
              "       [0.55448488],\n",
              "       [0.55730423],\n",
              "       [0.67112397],\n",
              "       [0.54830494],\n",
              "       [0.53499574],\n",
              "       [0.551601  ],\n",
              "       [0.55970495],\n",
              "       [0.55796883],\n",
              "       [0.67330631],\n",
              "       [0.74355842],\n",
              "       [0.52191099],\n",
              "       [0.72719999],\n",
              "       [0.71097163],\n",
              "       [0.53583446],\n",
              "       [0.55098907],\n",
              "       [0.54907603],\n",
              "       [0.71426781],\n",
              "       [0.55733072],\n",
              "       [0.54610846],\n",
              "       [0.55311164],\n",
              "       [0.55560611],\n",
              "       [0.56504977],\n",
              "       [0.55021941],\n",
              "       [0.55342624],\n",
              "       [0.55360664],\n",
              "       [0.75148109],\n",
              "       [0.72952474],\n",
              "       [0.73479455],\n",
              "       [0.7219738 ],\n",
              "       [0.56066847],\n",
              "       [0.54155373],\n",
              "       [0.55241205],\n",
              "       [0.58960578],\n",
              "       [0.73582519],\n",
              "       [0.55160535],\n",
              "       [0.67585399],\n",
              "       [0.53224837],\n",
              "       [0.73954997],\n",
              "       [0.55170554],\n",
              "       [0.55092074],\n",
              "       [0.5472108 ],\n",
              "       [0.73479455],\n",
              "       [0.55618993],\n",
              "       [0.73909212],\n",
              "       [0.71976974],\n",
              "       [0.55576244],\n",
              "       [0.55008221],\n",
              "       [0.5499553 ],\n",
              "       [0.5379549 ],\n",
              "       [0.55284175],\n",
              "       [0.53597249],\n",
              "       [0.73954997],\n",
              "       [0.71932547],\n",
              "       [0.74196924],\n",
              "       [0.5601129 ],\n",
              "       [0.70260861],\n",
              "       [0.55444674],\n",
              "       [0.55062933],\n",
              "       [0.56347774],\n",
              "       [0.52457695],\n",
              "       [0.55375781],\n",
              "       [0.51383671],\n",
              "       [0.74207589],\n",
              "       [0.54907603],\n",
              "       [0.7198246 ],\n",
              "       [0.70579688],\n",
              "       [0.55023828],\n",
              "       [0.7127012 ],\n",
              "       [0.68243369],\n",
              "       [0.68504484],\n",
              "       [0.71644284],\n",
              "       [0.73909212],\n",
              "       [0.56367546],\n",
              "       [0.52318487],\n",
              "       [0.71170995],\n",
              "       [0.54138986],\n",
              "       [0.60011687],\n",
              "       [0.71976974],\n",
              "       [0.73186634],\n",
              "       [0.72839572],\n",
              "       [0.75148109],\n",
              "       [0.67394412],\n",
              "       [0.63724484],\n",
              "       [0.5479489 ],\n",
              "       [0.73577231],\n",
              "       [0.73177442],\n",
              "       [0.55955007],\n",
              "       [0.53858166],\n",
              "       [0.650855  ],\n",
              "       [0.71976974],\n",
              "       [0.52850301],\n",
              "       [0.54967558],\n",
              "       [0.54266044],\n",
              "       [0.71644284],\n",
              "       [0.70292917],\n",
              "       [0.54686895],\n",
              "       [0.56567027],\n",
              "       [0.72678246],\n",
              "       [0.72734684],\n",
              "       [0.55747223],\n",
              "       [0.55322191],\n",
              "       [0.72719999],\n",
              "       [0.72178755],\n",
              "       [0.67210007],\n",
              "       [0.55889763],\n",
              "       [0.54549314],\n",
              "       [0.55335855],\n",
              "       [0.72734684],\n",
              "       [0.5244276 ],\n",
              "       [0.71293338],\n",
              "       [0.71411668],\n",
              "       [0.55610771],\n",
              "       [0.71934523],\n",
              "       [0.73340484],\n",
              "       [0.72682913],\n",
              "       [0.54336344],\n",
              "       [0.72587854],\n",
              "       [0.5466217 ],\n",
              "       [0.58092816],\n",
              "       [0.5431354 ],\n",
              "       [0.72682913],\n",
              "       [0.55314026],\n",
              "       [0.55492631],\n",
              "       [0.55721736],\n",
              "       [0.73172103],\n",
              "       [0.71976974],\n",
              "       [0.72740078],\n",
              "       [0.55727896],\n",
              "       [0.54132928],\n",
              "       [0.64407642],\n",
              "       [0.54495294],\n",
              "       [0.69820135],\n",
              "       [0.72734684],\n",
              "       [0.73536313],\n",
              "       [0.70343655],\n",
              "       [0.74196924],\n",
              "       [0.56539254]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tvMc0OqwmSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb49669-fd24-4169-f6fe-073416f1b483"
      },
      "source": [
        "# Training cross entropy cost (or log-loss)\n",
        "train_cross_entropy = model.cross_entropy(y_train, model.predict_proba(X_train))\n",
        "print(\"Cross entropy cost on training data:\", train_cross_entropy)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross entropy cost on training data: 0.5790524142783702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGViZYRDLcIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d811ee65-90a2-4487-fc49-05d2efd965f6"
      },
      "source": [
        "# Testing cross entropy cost (or log-loss)\n",
        "test_cross_entropy = model.cross_entropy(y_test, model.predict_proba(X_test))\n",
        "print(\"Cross entropy cost on testing data:\", test_cross_entropy)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross entropy cost on testing data: 0.59897286639796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vnjkAvzI5pN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4a30526a-0577-4d1e-f12f-70bc837bd1eb"
      },
      "source": [
        "# Plot the loss curve\n",
        "plt.plot([i+1 for i in range(len(model.cross_entropy_error))], model.cross_entropy_error)\n",
        "plt.title(\"Cross entropy error curve\")\n",
        "plt.xlabel(\"Iteration num\")\n",
        "plt.ylabel(\"Cross entropy (-ve log-likelihood)\")\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH-ocni8I5pO"
      },
      "source": [
        "##### Let's calculate accuracy as well. Accuracy is defined simply as the rate of correct classifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-RxIPbVI5pP"
      },
      "source": [
        "#Make predictions on test data\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsS3PX65I5pP"
      },
      "source": [
        "def accuracy(y_true,y_pred):\n",
        "    '''Compute accuracy.\n",
        "    Accuracy = (Correct prediction / number of samples)\n",
        "    Args:\n",
        "        y_true : Truth binary values (num_examples, )\n",
        "        y_pred : Predicted binary values (num_examples, )\n",
        "    Returns:\n",
        "        accuracy: scalar value\n",
        "    '''\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    correct_predictions = np.sum(y_true.T == y_pred)\n",
        "    num_of_samples = len(y_true)\n",
        "    accuracy = correct_predictions/num_of_samples\n",
        "    ### END CODE HERE\n",
        "    return accuracy"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05SA_Ur6I5pQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142fe1fd-58ac-445e-b9f7-bc098b06882f"
      },
      "source": [
        "# Print accuracy on train data\n",
        "print(accuracy(y_test,y_pred))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6153846153846154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqIUW3aPm_WC"
      },
      "source": [
        "#Make predictions on test data\n",
        "y_pred = model.predict(X_train)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ0zLpChI5pQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ac6fab-b31a-4cfc-e863-387a5ec969b6"
      },
      "source": [
        "# Print accuracy on test data\n",
        "print(accuracy(y_train,y_pred))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9027777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPU37tWAI5pR"
      },
      "source": [
        "## Part 1.2: Use Logistic Regression from sklearn on the same dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHscCs3bI5pR"
      },
      "source": [
        "#### Tasks\n",
        "- Define X and y again for sklearn Linear Regression model\n",
        "- Train Logistic Regression Model on the training set (sklearn.linear_model.LogisticRegression class)\n",
        "- Run the model on testing set\n",
        "- Print 'accuracy' obtained on the testing dataset (sklearn.metrics.accuracy_score function)\n",
        "\n",
        "#### Further fun (will not be evaluated)\n",
        "- Compare accuracies of your model and sklearn's logistic regression model\n",
        "\n",
        "#### Helpful links\n",
        "- Classification metrics in sklearn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcM8FakEI5pT"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP8AhS_vI5pT"
      },
      "source": [
        "# Define X and y\n",
        "X = df[:,:-1]\n",
        "y = df[:,-1:]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc7htjejI5pU"
      },
      "source": [
        "# Initialize the model from sklearn\n",
        "model = LogisticRegression()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owIrjceyI5pU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6108cc2e-a23c-4c4a-ffa6-d86d6f9967f1"
      },
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_YS9dn8I5pV"
      },
      "source": [
        "# Predict on testing set X_test\n",
        "y_pred = model.predict(X)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pBqDQ0qI5pW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a658b2-1f5d-4afd-f8af-c12b63060b54"
      },
      "source": [
        "# Print Accuracy on testing set\n",
        "test_accuracy_sklearn = accuracy_score(y,y_pred)\n",
        "\n",
        "print(f\"\\nAccuracy on testing set: {test_accuracy_sklearn}\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy on testing set: 0.9941176470588236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28hkMN_MI5pX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}